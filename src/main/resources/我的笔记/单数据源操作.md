#### 创建RDD的两种方法

- ```scala
  val rdd1: RDD[Int] = context.parallelize(List(1, 2, 3, 4))
  ```

- ```scala
  /*第二个参数是数据分的份数 ，需要在SparkConf中配置，如果没有配置则使用当前运行环境的最大CPU核数*/
  // scheduler.conf.getInt("spark.default.parallelism", totalCores)
  val rdd2: RDD[Int] = context.makeRDD(List(1, 2, 3, 4), 2)
  ```

#### RDD的分区算法

```scala
def positions(length: Long, numSlices: Int): Iterator[(Int, Int)] {
    (0 until numSlices).iterator.map { i =>
        val start = ((i * length) / numSlices).toInt
        val end = (((i + 1) * length) / numSlices).toInt
        (start, end)
    }
}
```

#### Hadoop 文件读取

1. 文件是一行一行的读取
2. 数据读取是以偏移量为单位，偏移量不会被重复读取
3. 如果数据源为多个文件，那么计算分区是一单个文件为单位

![image-20211202103929098](/home/ncse/snap/typora/42/.config/Typora/typora-user-images/image-20211202103929098.png)

#### RDD的执行

> 其中 map reduce等都是属于 转换算子
>
> 只有在RDD调用collect的时候才会触发任务的执行

```scala
val array:Array[(String,Int)] = wordToCount.collect()
```

#### Map

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val context = new SparkContext(sparkConf)
    val rdd2: RDD[Int] = context.makeRDD(List(1, 2, 3, 4))

    rdd2.map((num:Int)=>{num*2}).collect().foreach(println)

    context.stop()
```

#### RDD的并行计算

```scala
val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
val context = new SparkContext(sparkConf)


val rdd: RDD[Int] = context.makeRDD(List(1, 2, 3, 4))

val rdd1: RDD[Int] = rdd.map((num) => {
    println("************" + num)
    num
})
val rdd3: RDD[Int] = rdd1.map((num) => {
    println("===============" + num)
    num
})
rdd3.collect().foreach(println)
context.stop()
```

![image-20211202145602268](/home/ncse/snap/typora/42/.config/Typora/typora-user-images/image-20211202145602268.png)

> 如果两个算子之间的数据没有改变，那么这两个算子可以并行执行

#### RDD 分区Map

```scala
val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
val context = new SparkContext(sparkConf)


val rdd: RDD[Int] = context.makeRDD(List(1, 2, 3, 4),2)

// 可以以分区为单位进行数据转换操作
// 但是会将整个分区的数据加载到内存中进行应用
// 数据处理完不会被释放掉，存在对象的引用
// 在内存小，数据量较大的场合下。容易出现内存溢出问题
val value: RDD[Int] = rdd.mapPartitions(iter => {
    println("分区执行")
    iter.map(_ * 2)
})

value.collect()
context.stop()
```

![image-20211202162728739](/home/ncse/snap/typora/42/.config/Typora/typora-user-images/image-20211202162728739.png)

#### Glom

```scala
val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
val context = new SparkContext(sparkConf)


val rdd = context.makeRDD(List(1,2,3,4,5,6,7,8),2)

/*List => Int*/
/*Int => List*/
val glomRDD: RDD[Array[Int]] = rdd.glom()

glomRDD.collect().foreach(data =>println(data.mkString(",")))
context.stop()
```

#### Filter

```scala
val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
val context = new SparkContext(sparkConf)


//    val rdd = context.makeRDD(List(1,2,3,4,5,6,7,8),2)
//
//
//    val filterRDD: RDD[Int] = rdd.filter((num) => {
//      num % 2 == 1
//    })
//    filterRDD.collect().foreach(println)

val rdd: RDD[String] = context.textFile("src/main/resources/datas/apache.log")

val date: RDD[String] = rdd.filter(
    line => {
        val dates: Array[String] = line.split(" ")
        dates(3).startsWith("17/05/2015")
    }
)

date.collect().foreach(println)


context.stop()
```

#### Simple(抽样)

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val context = new SparkContext(sparkConf)


    val rdd = context.makeRDD(List(1,2,3,4,5,6,7,8,9,10));

    //sample 算子需要爱传递三个参数
    /*
    * 1. 第一个参数表示，抽取数据后是否将数据放回 true 返回 false 不放回
    *
    * 2. 第二个参数表示，数据源中每条数据被抽取的概率,
    					如果是抽取不放回的场合，数据源中的每条数据被抽取的概率，基准值的概念
    					如果是抽取放回的场合，表示我们的数据源中的每条数据被抽取的可能次数
    *
    * 3. 第三个参数表示，抽取数据时随机算法的种子,种子确定了随机值
    *                    如果不传递第三个参数，那么使用的当前的系统时间
    * */
    println(    rdd.sample(
      false,
      fraction = 0.4
    ).collect().mkString(","))

    context.stop()
```

#### distinct（去重）

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val context = new SparkContext(sparkConf)
    val rdd = context.makeRDD(List(1,2,3,4,1,2,3,4));
    val rddDistinct: RDD[Int] = rdd.distinct()
    rddDistinct.collect().foreach(println)
    context.stop()
```

#### Coalesce(缩减分区)

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val context = new SparkContext(sparkConf)

    /*
    根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率
    当Spark程序中，存在过多的小任务的时候，可以通过coalesce方法，收缩合并分区，减少分区的个数，减少任务的调度成本。
    * */
    val rdd = context.makeRDD(List(1,2,3,4),4);
    // coalesce 算子默认 不会打乱分区重新组合
   // 这种情况下的缩减分区可能导致数据不均衡，出现数据倾斜，如果想要让数据均衡，可以进行shuffle处理
   // coalesce 也可以扩大分区，前提是设置shuffle设置为true，否则不起作用
    val rdd2: RDD[Int] = rdd.coalesce(2)
    rdd2.saveAsTextFile("output")
    context.stop()
```

#### repartition（扩大分区）

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val context = new SparkContext(sparkConf)


    val rdd = context.makeRDD(List(1, 2, 3, 4, 5, 6), 3);
    // 扩大分区，底层实现还是Coalesce，主要适用于区分，便于理解，语法糖
    val value: RDD[Int] = rdd.repartition(2)
    value.saveAsTextFile("output")
    context.stop()
```

#### sortBy

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val context = new SparkContext(sparkConf)


    val rdd = context.makeRDD(List(("2",1),("11",2),("tets",5),("gzc",3)),2);

    /*sortBy 方法可以根据指定的规则对数据源中的数据进行排序，默认为升序，第二个参数可以设置为降序排列
    *
    * sortBy 默认情况下不会改变分区，但是中间存在shuffle操作
    * */
    val sortByRDD= rdd.sortBy(num => num._1,false)
    sortByRDD.collect().foreach(println)
    context.stop()
```

#### Key-Value 类型

###### partitionBy

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val context = new SparkContext(sparkConf)
    val rdd: RDD[Int] = context.makeRDD(List(1, 2, 3, 4))

    val keyValueRDD: RDD[(Int, Int)] = rdd.map((num) => {
      (num, 1)
    })
    /*partitionBy 根据指定的分区规则对数据进行重分区*/
    keyValueRDD.partitionBy(new HashPartitioner(2)).saveAsTextFile("output")
    context.stop()
```



###### groupByKey

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val context = new SparkContext(sparkConf)
    val rdd = context.makeRDD(List(
      ("a",1),
      ("a",2),
      ("a",3),
      ("b",4),
    ))
    /*groupByKey: 将数据源中的数据，相同key的数据分到一个组中，形成一个对偶元组
    *             元组中的第一个元素就是key
    *             元组中的第二个元素就是相同key对应的value集合
    * */
    val groupByKeyRDD: RDD[(String, Iterable[Int])] = rdd.groupByKey()
    groupByKeyRDD.collect().foreach(println)
```

###### reducebykey

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val context = new SparkContext(sparkConf)
    val rdd = context.makeRDD(List(
      ("a",1),
      ("a",2),
      ("a",3),
      ("b",4),
    ))
    /*reduceByKey 中如果key的数据只有一个，是不会进行聚合运算的
    *
    * shuffle 的时候Spark都会执行落盘操作，减少内存的使用量
    *
    * reduceByKey 支持分区内预聚合功能（combine），可以有效减少shuffle时落盘的数据量，提升shuffle的性能
    *
    * 所以reduceBykey 要比groupBykey的性能要好
    * */
    val reduceByKeyRDD: RDD[(String, Int)] = rdd.reduceByKey((x, y) => {
      x + y
    })
    reduceByKeyRDD.collect().foreach(println)
    context.stop()
```

###### aggregateByKey

```scala
//将数据根据不同的规则进行分区内计算和分区间计算
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val context = new SparkContext(sparkConf)
    val rdd = context.makeRDD(List(
      ("a",1),
      ("b",2),
      ("c",3),
      ("a",4),
      ("a",1),
      ("b",2),
      ("c",3),
      ("c",4),
    ),2)

    // 在计算机科学中，柯里化（Currying）是把接受多个参数的函数变换成接受一个单一参数(最初函数的第一个参数)的函数，
    // 并且返回接受余下的参数且返回结果的新函数的技术。
    // aggregateByKey 存在函数的柯里化
    // 第一个参数列表 需要传递一个参数，表示为初始值
    //                主要用于当遇到第一个Key的时候，和value进行分区内计算
    // 第二个参数列表
    //      第一个参数表示分区内计算规则
    //      第二个参数表示分区间计算规则
    val aggregateByKeyRDD: RDD[(String, Int)] = rdd.aggregateByKey(Int.MinValue)((x, y) => {
      math.max(x, y)
    },
      (x, y) => x + y
    )
    aggregateByKeyRDD.collect().foreach(println)
    context.stop()
```

###### 求平均值

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("RDD")
    val context = new SparkContext(sparkConf)
    val rdd = context.makeRDD(List(
      ("a",1),
      ("a",2),
      ("b",4),
      ("b",4),
      ("a",6)
    ),2)

    // 在计算机科学中，柯里化（Currying）是把接受多个参数的函数变换成接受一个单一参数(最初函数的第一个参数)的函数，
    // 并且返回接受余下的参数且返回结果的新函数的技术。
    // aggregateByKey 存在函数的柯里化
    // 第一个参数列表 需要传递一个参数，表示为初始值
    //                主要用于当遇到第一个Key的时候，和value进行分区内计算
    // 第二个参数列表
    //      第一个参数表示分区内计算规则
    //      第二个参数表示分区间计算规则
//    val aggregateByKeyRDD: RDD[(String, Int)] = rdd.aggregateByKey(Int.MinValue)((x, y) => {
//      math.max(x, y)
//    },
//      (x, y) => x + y
//    )
//    aggregateByKeyRDD.collect().foreach(println)
    val aggregateByKeyRDD: RDD[(String, (Int, Int))] = rdd.aggregateByKey((0, 0))(
      (t, v) => {
        (t._1 + v, t._2 + 1)
      },
      (t1, t2) => {
        (t1._1 + t2._1, t1._2 + t2._2)
      }
    )
    val value: RDD[(String, Int)] = aggregateByKeyRDD.map(
      (item) => {
        (item._1, item._2._1 / item._2._2)
      }
    )

    value.collect().foreach(println)

    context.stop()
```

#### 血缘关系

> 每执行一次算子都会在原来的血缘关系的基础上，添加一次依赖

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("血缘关系")
    val context = new SparkContext(sparkConf)

    val line: RDD[String] = context.textFile("src/main/resources/datas/1.txt")
    println(line.toDebugString)
    println("=================================================")
    val wordRDD: RDD[String] = line.flatMap(_.split(" "))
    println(wordRDD.toDebugString)
    println("=================================================")

    val wordToOne: RDD[(String, Int)] = wordRDD.map((_, 1))
    println(wordToOne.toDebugString)
    println("=================================================")

    val resultRDD: RDD[(String, Int)] = wordToOne.reduceByKey(_ + _)
    println(resultRDD.toDebugString)
    println("=================================================")

    resultRDD.collect().foreach(println)
    context.stop()
```

```shell
(2) src/main/resources/datas/1.txt MapPartitionsRDD[1] at textFile at Spark06_RDD_Operator_transform_KeyValue_dep.scala:13 []
 |  src/main/resources/datas/1.txt HadoopRDD[0] at textFile at Spark06_RDD_Operator_transform_KeyValue_dep.scala:13 []
=================================================
(2) MapPartitionsRDD[2] at flatMap at Spark06_RDD_Operator_transform_KeyValue_dep.scala:16 []
 |  src/main/resources/datas/1.txt MapPartitionsRDD[1] at textFile at Spark06_RDD_Operator_transform_KeyValue_dep.scala:13 []
 |  src/main/resources/datas/1.txt HadoopRDD[0] at textFile at Spark06_RDD_Operator_transform_KeyValue_dep.scala:13 []
=================================================
(2) MapPartitionsRDD[3] at map at Spark06_RDD_Operator_transform_KeyValue_dep.scala:20 []
 |  MapPartitionsRDD[2] at flatMap at Spark06_RDD_Operator_transform_KeyValue_dep.scala:16 []
 |  src/main/resources/datas/1.txt MapPartitionsRDD[1] at textFile at Spark06_RDD_Operator_transform_KeyValue_dep.scala:13 []
 |  src/main/resources/datas/1.txt HadoopRDD[0] at textFile at Spark06_RDD_Operator_transform_KeyValue_dep.scala:13 []
=================================================
(2) ShuffledRDD[4] at reduceByKey at Spark06_RDD_Operator_transform_KeyValue_dep.scala:24 []
 +-(2) MapPartitionsRDD[3] at map at Spark06_RDD_Operator_transform_KeyValue_dep.scala:20 []
    |  MapPartitionsRDD[2] at flatMap at Spark06_RDD_Operator_transform_KeyValue_dep.scala:16 []
    |  src/main/resources/datas/1.txt MapPartitionsRDD[1] at textFile at Spark06_RDD_Operator_transform_KeyValue_dep.scala:13 []
    |  src/main/resources/datas/1.txt HadoopRDD[0] at textFile at Spark06_RDD_Operator_transform_KeyValue_dep.scala:13 []
=================================================
```

#### 依赖关系

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("依赖关系")
    val context = new SparkContext(sparkConf)

    val line: RDD[String] = context.textFile("src/main/resources/datas/1.txt")
    println(line.dependencies)
    println("=================================================")
    val wordRDD: RDD[String] = line.flatMap(_.split(" "))
    println(wordRDD.dependencies)
    println("=================================================")

    val wordToOne: RDD[(String, Int)] = wordRDD.map((_, 1))
    println(wordToOne.dependencies)
    println("=================================================")

    val resultRDD: RDD[(String, Int)] = wordToOne.reduceByKey(_ + _)
    println(resultRDD.dependencies)
    println("=================================================")

    context.stop()
```

```shell
List(org.apache.spark.OneToOneDependency@2f84acf7)
=================================================
List(org.apache.spark.OneToOneDependency@142213d5)
=================================================
List(org.apache.spark.OneToOneDependency@ff23ae7)
=================================================
List(org.apache.spark.ShuffleDependency@21a02556)
=================================================
```

> OneToOneDependency 新的RDD的一个分区的数据依赖于旧的RDD一个分区的数据
>
> ShuffleDependency 新的RDD的一个分区的数据依赖于旧的RDD多个分区的数据

#### RDD持久化

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Persist")
    val context = new SparkContext(sparkConf)
	// 设置检查点的存储位置
	context.setCheckpointDir("backup")
    val rdd: RDD[String] = context.makeRDD(List("Hello Spark", "Hello Scala"))
    val wordRDD: RDD[String] = rdd.flatMap(_.split(" "))
    val wordToOneRDD: RDD[(String, Int)] = wordRDD.map(item =>{
      println("**************************")
      (item,1)
    })
    /**虽然复用了之前的RDD但是，底层还是执行了之前的RDD的生成操作，
     * 本质上还是没有复用，因为在RDD传给reducebykey之后当前的RDD已经从内存中释放了，
     * 如果想要复用RDD可以使用持久化的操作，持久化的操作包括两种
     1. 放到缓存中 不安全，可能导致内存溢出
     2. 放到文件中 效率低，因为有磁盘的IO操作
     3. 设置检查点
     */
	//1. 放到内存中方法
    wordToOneRDD.cache()
    //2. 放到磁盘中
    wordToOneRDD.persist(StorageLevel.DISK_ONLY)
	// 设置检查点
	wordToOneRDD.cache()
	wordToOneRDD.checkpoint()
	
    val resultRDD = wordToOneRDD.reduceByKey(_ + _)
    resultRDD.collect().foreach(println)
    val groupRDD: RDD[(String, Iterable[Int])] = wordToOneRDD.groupByKey()
    groupRDD.collect().foreach(println)
```

```shell
**************************
**************************
**************************
**************************
(Hello,2)
(Spark,1)
(Scala,1)
(Hello,CompactBuffer(1, 1))
(Spark,CompactBuffer(1))
(Scala,CompactBuffer(1))
```

> 三者的异同点
>
> 1. cache 将数据临时存储在内存中进行数据的重用，数据是不安全的，会在血缘关系中添加新的依赖。
> 2. persist 将数据存储在临时文件中进行数据重用，因为有IO操作，性能较低，但是数据安全 ，如果作业执行结束，临时文件会丢失。
> 3. checkpoint ：将数据长久保存在磁盘文件中，数据安全，性能最低，一般和cache一起使用，会切断血缘关系，重新建立新的血缘关系，等同于改变数据源。

|            | 安全性 |                    效率                     |
| :--------: | :----: | :-----------------------------------------: |
|   cache    | 不安全 |                   效率高                    |
|  persist   |  安全  |                   效率低                    |
| checkpoint |  安全  | 效率最低（如果和cache一起使用可以提高效率） |

#### 自定义分区器

```scala
  def main(args: Array[String]): Unit = {
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Persist")
    val context = new SparkContext(sparkConf)
    val rdd: RDD[(String, String)] = context.makeRDD(List(
      ("nba", "1"),
      ("cba", "1"),
      ("wnba", "1"),
      ("wcba", "1")
    ));
    val partRDD: RDD[(String, String)] = rdd.partitionBy(new Mypartitioner)

    partRDD.saveAsTextFile("output")

    context.stop()
  }

  /**
   * 自定义分区器
   */
  class Mypartitioner extends Partitioner {
    /**
     * 分区的数量
     *
     * @return
     */
    override def numPartitions: Int = 3

    //返回数据的分区索引
    override def getPartition(key: Any): Int = {
      key match {
        case "nba" => 0
        case "wnba" => 1
        case _ => 2

      }
    }
  }
```

#### RDD IO操作

###### 写操作

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Save")
    val context = new SparkContext(sparkConf)
    val rdd: RDD[(String, Int)] = context.makeRDD(List(
      ("a", 1),
      ("b", 2),
      ("c", 3)
    ))

    rdd.saveAsTextFile("output/output1")
    rdd.saveAsObjectFile("output/output2")
    rdd.saveAsSequenceFile("output/output3")

    context.stop()
```

###### 读操作

```scala
    val sparkConf: SparkConf = new SparkConf().setMaster("local[*]").setAppName("Save")
    val context = new SparkContext(sparkConf)
    val rdd1 = context.textFile("output/output1")
    rdd1.collect().foreach(println)

    val rdd2: RDD[(String,Int)] = context.objectFile[(String,Int)]("output/output2")
    rdd2.collect().foreach(println)

    val rdd3: RDD[(String, Int)] = context.sequenceFile[String, Int]("output/output3")
    rdd3.collect().foreach(println)
    context.stop()
```

